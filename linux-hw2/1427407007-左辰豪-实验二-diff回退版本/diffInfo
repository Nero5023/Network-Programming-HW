4c4
< #         Shiqiao Du <lucidfrontier.45@gmail.com>
---
> # and Shiqiao Du <lucidfrontier.45@gmail.com>
7d6
< # More API changes: Sergei Lebedev <superbobry@gmail.com>
12a12,13
> import string
> 
14c15
< from sklearn import cluster
---
> from sklearn.utils import check_random_state
17d17
<     log_multivariate_normal_density,
19,22c19
< from sklearn.utils import check_random_state
< 
< from .base import _BaseHMM
< from .utils import iter_from_X_lengths, normalize
---
> from sklearn import cluster
24c21,31
< __all__ = ["GMMHMM", "GaussianHMM", "MultinomialHMM"]
---
> from .base import _BaseHMM, decoder_algorithms
> from .utils import normalize
> from .utils.fixes import log_multivariate_normal_density
> 
> __all__ = ['GMMHMM',
>            'GaussianHMM',
>            'MultinomialHMM',
> 
>            # for compatbility, but we should remove this, really.
>            'decoder_algorithms',
>            'normalize']
26c33
< COVARIANCE_TYPES = frozenset(("spherical", "diag", "full", "tied"))
---
> NEGINF = -np.inf
30c37,41
<     """Hidden Markov Model with Gaussian emissions.
---
>     """Hidden Markov Model with Gaussian emissions
> 
>     Representation of a hidden Markov model probability distribution.
>     This class allows for easy evaluation of, sampling from, and
>     maximum-likelihood estimation of the parameters of a HMM.
39c50,57
<         use.  Must be one of
---
>         use.  Must be one of 'spherical', 'tied', 'diag', 'full'.
>         Defaults to 'diag'.
> 
>     Attributes
>     ----------
>     _covariance_type : string
>         String describing the type of covariance parameters used by
>         the model.  Must be one of 'spherical', 'tied', 'diag', 'full'.
41,46c59,60
<         * "spherical" --- each state uses a single variance value that
<           applies to all features;
<         * "diag" --- each state uses a diagonal covariance matrix;
<         * "full" --- each state uses a full (i.e. unrestricted)
<           covariance matrix;
<         * "tied" --- all states use **the same** full covariance matrix.
---
>     n_features : int
>         Dimensionality of the Gaussian emissions.
48c62,63
<         Defaults to "diag".
---
>     n_components : int
>         Number of states in the model.
50,52c65,66
<     min_covar : float
<         Floor on the diagonal of the covariance matrix to prevent
<         overfitting. Defaults to 1e-3.
---
>     transmat : array, shape (`n_components`, `n_components`)
>         Matrix of transition probabilities between states.
54,55c68,69
<     startprob_prior : array, shape (n_components, )
<         Initial state occupation prior distribution.
---
>     startprob : array, shape ('n_components`,)
>         Initial state occupation distribution.
57,58c71,72
<     transmat_prior : array, shape (n_components, n_components)
<         Matrix of prior transition probabilities between states.
---
>     means : array, shape (`n_components`, `n_features`)
>         Mean parameters for each state.
60,62c74,81
<     algorithm : string
<         Decoder algorithm. Must be one of "viterbi" or "map".
<         Defaults to "viterbi".
---
>     covars : array
>         Covariance parameters for each state.  The shape depends on
>         ``_covariance_type``::
> 
>             (`n_components`,)                   if 'spherical',
>             (`n_features`, `n_features`)              if 'tied',
>             (`n_components`, `n_features`)           if 'diag',
>             (`n_components`, `n_features`, `n_features`)  if 'full'
64,65c83,84
<     random_state: RandomState or an int seed
<         A random number generator instance.
---
>     random_state: RandomState or an int seed (0 by default)
>         A random number generator instance
68c87
<         Maximum number of iterations to perform.
---
>         Number of iterations to perform.
70,77c89,90
<     tol : float, optional
<         Convergence threshold. EM will stop if the gain in log-likelihood
<         is below this value.
< 
<     verbose : bool, optional
<         When ``True`` per-iteration convergence reports are printed
<         to :data:`sys.stderr`. You can diagnose convergence via the
<         :attr:`monitor_` attribute.
---
>     thresh : float, optional
>         Convergence threshold.
82,83c95,96
<         't' for transmat, 'm' for means and 'c' for covars. Defaults
<         to all parameters.
---
>         't' for transmat, 'm' for means, and 'c' for covars.
>         Defaults to all parameters.
88,89c101,102
<         startprob, 't' for transmat, 'm' for means and 'c' for covars.
<         Defaults to all parameters.
---
>         startprob, 't' for transmat, 'm' for means, and 'c' for
>         covars.  Defaults to all parameters.
91,116d103
<     Attributes
<     ----------
<     n_features : int
<         Dimensionality of the Gaussian emissions.
< 
<     monitor\_ : ConvergenceMonitor
<         Monitor object used to check the convergence of EM.
< 
<     transmat\_ : array, shape (n_components, n_components)
<         Matrix of transition probabilities between states.
< 
<     startprob\_ : array, shape (n_components, )
<         Initial state occupation distribution.
< 
<     means\_ : array, shape (n_components, n_features)
<         Mean parameters for each state.
< 
<     covars\_ : array
<         Covariance parameters for each state.
< 
<         The shape depends on ``covariance_type``::
< 
<             (n_components, )                        if 'spherical',
<             (n_features, n_features)                if 'tied',
<             (n_components, n_features)              if 'diag',
<             (n_components, n_features, n_features)  if 'full'
123a111,115
> 
> 
>     See Also
>     --------
>     GMM : Gaussian mixture model
125,128c117,120
<     def __init__(self, n_components=1, covariance_type='diag',
<                  min_covar=1e-3,
<                  startprob_prior=1.0, transmat_prior=1.0,
<                  means_prior=0, means_weight=0,
---
> 
>     def __init__(self, n_components=1, covariance_type='diag', startprob=None,
>                  transmat=None, startprob_prior=None, transmat_prior=None,
>                  algorithm="viterbi", means_prior=None, means_weight=0,
130,133c122,125
<                  algorithm="viterbi", random_state=None,
<                  n_iter=10, tol=1e-2, verbose=False,
<                  params="stmc", init_params="stmc"):
<         _BaseHMM.__init__(self, n_components,
---
>                  random_state=None, n_iter=10, thresh=1e-2,
>                  params=string.ascii_letters,
>                  init_params=string.ascii_letters):
>         _BaseHMM.__init__(self, n_components, startprob, transmat,
137c129
<                           tol=tol, params=params, verbose=verbose,
---
>                           thresh=thresh, params=params,
140,141c132,135
<         self.covariance_type = covariance_type
<         self.min_covar = min_covar
---
>         self._covariance_type = covariance_type
>         if covariance_type not in ['spherical', 'tied', 'diag', 'full']:
>             raise ValueError('bad covariance_type')
> 
143a138
> 
146a142,164
>     @property
>     def covariance_type(self):
>         """Covariance type of the model.
> 
>         Must be one of 'spherical', 'tied', 'diag', 'full'.
>         """
>         return self._covariance_type
> 
>     def _get_means(self):
>         """Mean parameters for each state."""
>         return self._means_
> 
>     def _set_means(self, means):
>         means = np.asarray(means)
>         if (hasattr(self, 'n_features')
>                 and means.shape != (self.n_components, self.n_features)):
>             raise ValueError('means must have shape '
>                              '(n_components, n_features)')
>         self._means_ = means.copy()
>         self.n_features = self._means_.shape[1]
> 
>     means_ = property(_get_means, _set_means)
> 
149c167
<         if self.covariance_type == 'full':
---
>         if self._covariance_type == 'full':
151,157c169,174
<         elif self.covariance_type == 'diag':
<             return np.array([np.diag(cov) for cov in self._covars_])
<         elif self.covariance_type == 'tied':
<             return np.array([self._covars_] * self.n_components)
<         elif self.covariance_type == 'spherical':
<             return np.array(
<                 [np.eye(self.n_features) * cov for cov in self._covars_])
---
>         elif self._covariance_type == 'diag':
>             return [np.diag(cov) for cov in self._covars_]
>         elif self._covariance_type == 'tied':
>             return [self._covars_] * self.n_components
>         elif self._covariance_type == 'spherical':
>             return [np.eye(self.n_features) * f for f in self._covars_]
160c177,179
<         self._covars_ = np.asarray(covars).copy()
---
>         covars = np.asarray(covars)
>         _validate_covars(covars, self._covariance_type, self.n_components)
>         self._covars_ = covars.copy()
164,172c183,185
<     def _check(self):
<         super(GaussianHMM, self)._check()
< 
<         self.means_ = np.asarray(self.means_)
<         self.n_features = self.means_.shape[1]
< 
<         if self.covariance_type not in COVARIANCE_TYPES:
<             raise ValueError('covariance_type must be one of {0}'
<                              .format(COVARIANCE_TYPES))
---
>     def _compute_log_likelihood(self, obs):
>         return log_multivariate_normal_density(
>             obs, self._means_, self._covars_, self._covariance_type)
174,175c187,193
<         _validate_covars(self._covars_, self.covariance_type,
<                          self.n_components)
---
>     def _generate_sample_from_state(self, state, random_state=None):
>         if self._covariance_type == 'tied':
>             cv = self._covars_
>         else:
>             cv = self._covars_[state]
>         return sample_gaussian(self._means_[state], cv, self._covariance_type,
>                                random_state=random_state)
177,178c195,196
<     def _init(self, X, lengths=None):
<         super(GaussianHMM, self)._init(X, lengths=lengths)
---
>     def _init(self, obs, params='stmc'):
>         super(GaussianHMM, self)._init(obs, params=params)
180,181c198,199
<         _, n_features = X.shape
<         if hasattr(self, 'n_features') and self.n_features != n_features:
---
>         if (hasattr(self, 'n_features')
>                 and self.n_features != obs[0].shape[1]):
183c201,204
<                              'expected %s' % (n_features, self.n_features))
---
>                              'expected %s' % (obs[0].shape[1],
>                                               self.n_features))
> 
>         self.n_features = obs[0].shape[1]
185,192c206,210
<         self.n_features = n_features
<         if 'm' in self.init_params or not hasattr(self, "means_"):
<             kmeans = cluster.KMeans(n_clusters=self.n_components,
<                                     random_state=self.random_state)
<             kmeans.fit(X)
<             self.means_ = kmeans.cluster_centers_
<         if 'c' in self.init_params or not hasattr(self, "covars_"):
<             cv = np.cov(X.T) + self.min_covar * np.eye(X.shape[1])
---
>         if 'm' in params:
>             self._means_ = cluster.KMeans(
>                 n_clusters=self.n_components).fit(obs[0]).cluster_centers_
>         if 'c' in params:
>             cv = np.cov(obs[0].T)
196,208c214,215
<                 cv, self.covariance_type, self.n_components).copy()
< 
<     def _compute_log_likelihood(self, X):
<         return log_multivariate_normal_density(
<             X, self.means_, self._covars_, self.covariance_type)
< 
<     def _generate_sample_from_state(self, state, random_state=None):
<         if self.covariance_type == 'tied':
<             cv = self._covars_
<         else:
<             cv = self._covars_[state]
<         return sample_gaussian(self.means_[state], cv, self.covariance_type,
<                                random_state=random_state)
---
>                 cv, self._covariance_type, self.n_components)
>             self._covars_[self._covars_ == 0] = 1e-5
215c222
<         if self.covariance_type in ('tied', 'full'):
---
>         if self._covariance_type in ('tied', 'full'):
217c224
<                                            self.n_features))
---
>                                           self.n_features))
221c228,229
<                                           posteriors, fwdlattice, bwdlattice):
---
>                                           posteriors, fwdlattice, bwdlattice,
>                                           params):
223c231,232
<             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice)
---
>             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice,
>             params)
225c234
<         if 'm' in self.params or 'c' in self.params:
---
>         if 'm' in params or 'c' in params:
229,230c238,239
<         if 'c' in self.params:
<             if self.covariance_type in ('spherical', 'diag'):
---
>         if 'c' in params:
>             if self._covariance_type in ('spherical', 'diag'):
232,236c241,245
<             elif self.covariance_type in ('tied', 'full'):
<                 # posteriors: (nt, nc); obs: (nt, nf); obs: (nt, nf)
<                 # -> (nc, nf, nf)
<                 stats['obs*obs.T'] += np.einsum(
<                     'ij,ik,il->jkl', posteriors, obs, obs)
---
>             elif self._covariance_type in ('tied', 'full'):
>                 for t, o in enumerate(obs):
>                     obsobsT = np.outer(o, o)
>                     for c in range(self.n_components):
>                         stats['obs*obs.T'][c] += posteriors[t, c] * obsobsT
238,239c247,248
<     def _do_mstep(self, stats):
<         super(GaussianHMM, self)._do_mstep(stats)
---
>     def _do_mstep(self, stats, params):
>         super(GaussianHMM, self)._do_mstep(stats, params)
241,245d249
<         means_prior = self.means_prior
<         means_weight = self.means_weight
< 
<         # TODO: find a proper reference for estimates for different
<         #       covariance models.
249,251c253,259
<         if 'm' in self.params:
<             self.means_ = ((means_weight * means_prior + stats['obs'])
<                            / (means_weight + denom))
---
>         if 'm' in params:
>             prior = self.means_prior
>             weight = self.means_weight
>             if prior is None:
>                 weight = 0
>                 prior = 0
>             self._means_ = (weight * prior + stats['obs']) / (weight + denom)
253c261
<         if 'c' in self.params:
---
>         if 'c' in params:
256c264,273
<             meandiff = self.means_ - means_prior
---
>             if covars_prior is None:
>                 covars_weight = 0
>                 covars_prior = 0
> 
>             means_prior = self.means_prior
>             means_weight = self.means_weight
>             if means_prior is None:
>                 means_weight = 0
>                 means_prior = 0
>             meandiff = self._means_ - means_prior
258,259c275,276
<             if self.covariance_type in ('spherical', 'diag'):
<                 cv_num = (means_weight * meandiff**2
---
>             if self._covariance_type in ('spherical', 'diag'):
>                 cv_num = (means_weight * (meandiff) ** 2
261,262c278,279
<                           - 2 * self.means_ * stats['obs']
<                           + self.means_**2 * denom)
---
>                           - 2 * self._means_ * stats['obs']
>                           + self._means_ ** 2 * denom)
264,266c281,282
<                 self._covars_ = \
<                     (covars_prior + cv_num) / np.maximum(cv_den, 1e-5)
<                 if self.covariance_type == 'spherical':
---
>                 self._covars_ = (covars_prior + cv_num) / np.maximum(cv_den, 1e-5)
>                 if self._covariance_type == 'spherical':
270,271c286,287
<             elif self.covariance_type in ('tied', 'full'):
<                 cv_num = np.empty((self.n_components, self.n_features,
---
>             elif self._covariance_type in ('tied', 'full'):
>                 cvnum = np.empty((self.n_components, self.n_features,
274c290
<                     obsmean = np.outer(stats['obs'][c], self.means_[c])
---
>                     obsmean = np.outer(stats['obs'][c], self._means_[c])
276,281c292,297
<                     cv_num[c] = (means_weight * np.outer(meandiff[c],
<                                                          meandiff[c])
<                                  + stats['obs*obs.T'][c]
<                                  - obsmean - obsmean.T
<                                  + np.outer(self.means_[c], self.means_[c])
<                                  * stats['post'][c])
---
>                     cvnum[c] = (means_weight * np.outer(meandiff[c],
>                                                         meandiff[c])
>                                 + stats['obs*obs.T'][c]
>                                 - obsmean - obsmean.T
>                                 + np.outer(self._means_[c], self._means_[c])
>                                 * stats['post'][c])
283,284c299,300
<                 if self.covariance_type == 'tied':
<                     self._covars_ = ((covars_prior + cv_num.sum(axis=0)) /
---
>                 if self._covariance_type == 'tied':
>                     self._covars_ = ((covars_prior + cvnum.sum(axis=0)) /
286,287c302,303
<                 elif self.covariance_type == 'full':
<                     self._covars_ = ((covars_prior + cv_num) /
---
>                 elif self._covariance_type == 'full':
>                     self._covars_ = ((covars_prior + cvnum) /
289a306,329
>     def fit(self, obs):
>         """Estimate model parameters.
> 
>         An initialization step is performed before entering the EM
>         algorithm. If you want to avoid this step, pass proper
>         ``init_params`` keyword argument to estimator's constructor.
> 
>         Parameters
>         ----------
>         obs : list
>             List of array-like observation sequences, each of which
>             has shape (n_i, n_features), where n_i is the length of
>             the i_th observation.
> 
>         Notes
>         -----
>         In general, `logprob` should be non-decreasing unless
>         aggressive pruning is used.  Decreasing `logprob` is generally
>         a sign of overfitting (e.g. the covariance parameter on one or
>         more components becomminging too small).  You can fix this by getting
>         more training data, or increasing covars_prior.
>         """
>         return super(GaussianHMM, self).fit(obs)
> 
294c334
<     Parameters
---
>     Attributes
296d335
< 
298c337
<         Number of states.
---
>         Number of states in the model.
300,301c339,340
<     startprob_prior : array, shape (n_components, )
<         Initial state occupation prior distribution.
---
>     n_symbols : int
>         Number of possible symbols emitted by the model (in the observations).
303,304c342,343
<     transmat_prior : array, shape (n_components, n_components)
<         Matrix of prior transition probabilities between states.
---
>     transmat : array, shape (`n_components`, `n_components`)
>         Matrix of transition probabilities between states.
306,308c345,346
<     algorithm : string
<         Decoder algorithm. Must be one of "viterbi" or "map".
<         Defaults to "viterbi".
---
>     startprob : array, shape ('n_components`,)
>         Initial state occupation distribution.
310,311c348,352
<     random_state: RandomState or an int seed
<         A random number generator instance.
---
>     emissionprob : array, shape ('n_components`, 'n_symbols`)
>         Probability of emitting a given symbol when in each state.
> 
>     random_state: RandomState or an int seed (0 by default)
>         A random number generator instance
314c355
<         Maximum number of iterations to perform.
---
>         Number of iterations to perform.
316,323c357,358
<     tol : float, optional
<         Convergence threshold. EM will stop if the gain in log-likelihood
<         is below this value.
< 
<     verbose : bool, optional
<         When ``True`` per-iteration convergence reports are printed
<         to :data:`sys.stderr`. You can diagnose convergence via the
<         :attr:`monitor_` attribute.
---
>     thresh : float, optional
>         Convergence threshold.
328c363
<         't' for transmat, 'e' for emissionprob.
---
>         't' for transmat, 'e' for emmissionprob.
334c369
<         startprob, 't' for transmat, 'e' for emissionprob.
---
>         startprob, 't' for transmat, 'e' for emmissionprob.
337,353d371
<     Attributes
<     ----------
<     n_features : int
<         Number of possible symbols emitted by the model (in the samples).
< 
<     monitor\_ : ConvergenceMonitor
<         Monitor object used to check the convergence of EM.
< 
<     transmat\_ : array, shape (n_components, n_components)
<         Matrix of transition probabilities between states.
< 
<     startprob\_ : array, shape (n_components, )
<         Initial state occupation distribution.
< 
<     emissionprob\_ : array, shape (n_components, n_features)
<         Probability of emitting a given symbol when in each state.
< 
359a378,381
> 
>     See Also
>     --------
>     GaussianHMM : HMM with Gaussian emissions
362,363c384,385
<     def __init__(self, n_components=1,
<                  startprob_prior=1.0, transmat_prior=1.0,
---
>     def __init__(self, n_components=1, startprob=None, transmat=None,
>                  startprob_prior=None, transmat_prior=None,
365,367c387,396
<                  n_iter=10, tol=1e-2, verbose=False,
<                  params="ste", init_params="ste"):
<         _BaseHMM.__init__(self, n_components,
---
>                  n_iter=10, thresh=1e-2, params=string.ascii_letters,
>                  init_params=string.ascii_letters):
>         """Create a hidden Markov model with multinomial emissions.
> 
>         Parameters
>         ----------
>         n_components : int
>             Number of states.
>         """
>         _BaseHMM.__init__(self, n_components, startprob, transmat,
372,378c401,404
<                           n_iter=n_iter, tol=tol, verbose=verbose,
<                           params=params, init_params=init_params)
< 
<     def _init(self, X, lengths=None):
<         if not self._check_input_symbols(X):
<             raise ValueError("expected a sample from "
<                              "a Multinomial distribution.")
---
>                           n_iter=n_iter,
>                           thresh=thresh,
>                           params=params,
>                           init_params=init_params)
380,381c406,425
<         super(MultinomialHMM, self)._init(X, lengths=lengths)
<         self.random_state = check_random_state(self.random_state)
---
>     def _get_emissionprob(self):
>         """Emission probability distribution for each state."""
>         return np.exp(self._log_emissionprob)
> 
>     def _set_emissionprob(self, emissionprob):
>         emissionprob = np.asarray(emissionprob)
>         if hasattr(self, 'n_symbols') and \
>                 emissionprob.shape != (self.n_components, self.n_symbols):
>             raise ValueError('emissionprob must have shape '
>                              '(n_components, n_symbols)')
> 
>         # check if there exists a component whose value is exactly zero
>         # if so, add a small number and re-normalize
>         if not np.alltrue(emissionprob):
>             normalize(emissionprob)
> 
>         self._log_emissionprob = np.log(emissionprob)
>         underflow_idx = np.isnan(self._log_emissionprob)
>         self._log_emissionprob[underflow_idx] = NEGINF
>         self.n_symbols = self._log_emissionprob.shape[1]
383,402c427
<         if 'e' in self.init_params:
<             if not hasattr(self, "n_features"):
<                 symbols = set()
<                 for i, j in iter_from_X_lengths(X, lengths):
<                     symbols |= set(X[i:j].flatten())
<                 self.n_features = len(symbols)
<             self.emissionprob_ = self.random_state \
<                 .rand(self.n_components, self.n_features)
<             normalize(self.emissionprob_, axis=1)
< 
<     def _check(self):
<         super(MultinomialHMM, self)._check()
< 
<         self.emissionprob_ = np.atleast_2d(self.emissionprob_)
<         n_features = getattr(self, "n_features", self.emissionprob_.shape[1])
<         if self.emissionprob_.shape != (self.n_components, n_features):
<             raise ValueError(
<                 "emissionprob_ must have shape (n_components, n_features)")
<         else:
<             self.n_features = n_features
---
>     emissionprob_ = property(_get_emissionprob, _set_emissionprob)
404,405c429,430
<     def _compute_log_likelihood(self, X):
<         return np.log(self.emissionprob_)[:, np.concatenate(X)].T
---
>     def _compute_log_likelihood(self, obs):
>         return self._log_emissionprob[:, obs].T
410c435,451
<         return [(cdf > random_state.rand()).argmax()]
---
>         rand = random_state.rand()
>         symbol = (cdf > rand).argmax()
>         return symbol
> 
>     def _init(self, obs, params='ste'):
>         super(MultinomialHMM, self)._init(obs, params=params)
>         self.random_state = check_random_state(self.random_state)
> 
>         if 'e' in params:
>             if not hasattr(self, 'n_symbols'):
>                 symbols = set()
>                 for o in obs:
>                     symbols = symbols.union(set(o))
>                 self.n_symbols = len(symbols)
>             emissionprob = normalize(self.random_state.rand(self.n_components,
>                                                             self.n_symbols), 1)
>             self.emissionprob_ = emissionprob
414c455
<         stats['obs'] = np.zeros((self.n_components, self.n_features))
---
>         stats['obs'] = np.zeros((self.n_components, self.n_symbols))
417,418c458,460
<     def _accumulate_sufficient_statistics(self, stats, X, framelogprob,
<                                           posteriors, fwdlattice, bwdlattice):
---
>     def _accumulate_sufficient_statistics(self, stats, obs, framelogprob,
>                                           posteriors, fwdlattice, bwdlattice,
>                                           params):
420,422c462,465
<             stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)
<         if 'e' in self.params:
<             for t, symbol in enumerate(np.concatenate(X)):
---
>             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice,
>             params)
>         if 'e' in params:
>             for t, symbol in enumerate(obs):
425,427c468,470
<     def _do_mstep(self, stats):
<         super(MultinomialHMM, self)._do_mstep(stats)
<         if 'e' in self.params:
---
>     def _do_mstep(self, stats, params):
>         super(MultinomialHMM, self)._do_mstep(stats, params)
>         if 'e' in params:
431,432c474,475
<     def _check_input_symbols(self, X):
<         """Check if ``X`` is a sample from a Multinomial distribution.
---
>     def _check_input_symbols(self, obs):
>         """Check if ``obs`` is a sample from a Multinomial distribution.
434,436c477,479
<         That is ``X`` should be an array of non-negative integers from
<         range ``[min(X), max(X)]``, such that each integer from the range
<         occurs in ``X`` at least once.
---
>         That is ``obs`` should be an array of non-negative integers from
>         range ``[min(obs), max(obs)]``, such that each integer from the range
>         occurs in ``obs`` at least once.
441c484
<         symbols = np.concatenate(X)
---
>         symbols = np.concatenate(obs)
444c487
<             (symbols < 0).any()):         # contains negative integers
---
>             np.any(symbols < 0)):         # contains negative integers
449a493,494
>     def fit(self, obs, **kwargs):
>         """Estimate model parameters.
451,483c496,509
< class GMMHMM(_BaseHMM):
<     """Hidden Markov Model with Gaussian mixture emissions.
< 
<     Parameters
<     ----------
<     n_components : int
<         Number of states in the model.
< 
<     n_mix : int
<         Number of states in the GMM.
< 
<     covariance_type : string
<         String describing the type of covariance parameters to
<         use.  Must be one of
< 
<         * "spherical" --- each state uses a single variance value that
<           applies to all features;
<         * "diag" --- each state uses a diagonal covariance matrix;
<         * "full" --- each state uses a full (i.e. unrestricted)
<           covariance matrix;
<         * "tied" --- all states use **the same** full covariance matrix.
< 
<         Defaults to "diag".
< 
<     startprob_prior : array, shape (n_components, )
<         Initial state occupation prior distribution.
< 
<     transmat_prior : array, shape (n_components, n_components)
<         Matrix of prior transition probabilities between states.
< 
<     algorithm : string
<         Decoder algorithm. Must be one of "viterbi" or "map".
<         Defaults to "viterbi".
---
>         An initialization step is performed before entering the EM
>         algorithm. If you want to avoid this step, pass proper
>         ``init_params`` keyword argument to estimator's constructor.
> 
>         Parameters
>         ----------
>         obs : list
>             List of array-like observation sequences. Each observation
>             sequence should consist of two or more integers from
>             range ``[0, n_symbols - 1]``.
>         """
>         if not self._check_input_symbols(obs):
>             raise ValueError("expected a sample from "
>                              "a Multinomial distribution.")
485,486c511
<     random_state: RandomState or an int seed
<         A random number generator instance.
---
>         return _BaseHMM.fit(self, obs, **kwargs)
488,489d512
<     n_iter : int, optional
<         Maximum number of iterations to perform.
491,498c514,515
<     tol : float, optional
<         Convergence threshold. EM will stop if the gain in log-likelihood
<         is below this value.
< 
<     verbose : bool, optional
<         When ``True`` per-iteration convergence reports are printed
<         to :data:`sys.stderr`. You can diagnose convergence via the
<         :attr:`monitor_` attribute.
---
> class GMMHMM(_BaseHMM):
>     """Hidden Markov Model with Gaussin mixture emissions
499a517,518
>     Attributes
>     ----------
512,518c531,532
<     Attributes
<     ----------
<     monitor\_ : ConvergenceMonitor
<         Monitor object used to check the convergence of EM.
< 
<     startprob\_ : array, shape (n_components, )
<         Initial state occupation distribution.
---
>     n_components : int
>         Number of states in the model.
520c534
<     transmat\_ : array, shape (n_components, n_components)
---
>     transmat : array, shape (`n_components`, `n_components`)
523c537,540
<     gmms\_ : list of GMM objects, length n_components
---
>     startprob : array, shape ('n_components`,)
>         Initial state occupation distribution.
> 
>     gmms : array of GMM objects, length `n_components`
525a543,551
>     random_state : RandomState or an int seed (0 by default)
>         A random number generator instance
> 
>     n_iter : int, optional
>         Number of iterations to perform.
> 
>     thresh : float, optional
>         Convergence threshold.
> 
531a558,561
> 
>     See Also
>     --------
>     GaussianHMM : HMM with Gaussian emissions
534,540c564,577
<     def __init__(self, n_components=1, n_mix=1,
<                  startprob_prior=1.0, transmat_prior=1.0,
<                  covariance_type='diag', covars_prior=1e-2,
<                  algorithm="viterbi", random_state=None,
<                  n_iter=10, tol=1e-2, verbose=False,
<                  params="stmcw", init_params="stmcw"):
<         _BaseHMM.__init__(self, n_components,
---
>     def __init__(self, n_components=1, n_mix=1, startprob=None, transmat=None,
>                  startprob_prior=None, transmat_prior=None,
>                  algorithm="viterbi", gmms=None, covariance_type='diag',
>                  covars_prior=1e-2, random_state=None, n_iter=10, thresh=1e-2,
>                  params=string.ascii_letters,
>                  init_params=string.ascii_letters):
>         """Create a hidden Markov model with GMM emissions.
> 
>         Parameters
>         ----------
>         n_components : int
>             Number of states.
>         """
>         _BaseHMM.__init__(self, n_components, startprob, transmat,
543,551c580,585
<                           algorithm=algorithm, random_state=random_state,
<                           n_iter=n_iter, tol=tol, verbose=verbose,
<                           params=params, init_params=init_params)
< 
<         if covariance_type != "diag":
<             warnings.warn("Fitting a GMMHMM with {0!r} covariance type "
<                           "is broken in 0.2.0. Please update to 0.2.1 once "
<                           "it's available.".format(covariance_type),
<                           UserWarning)
---
>                           algorithm=algorithm,
>                           random_state=random_state,
>                           n_iter=n_iter,
>                           thresh=thresh,
>                           params=params,
>                           init_params=init_params)
556c590
<         self.covariance_type = covariance_type
---
>         self._covariance_type = covariance_type
558,568c592,606
<         self.gmms_ = []
<         for x in range(self.n_components):
<             if covariance_type is None:
<                 gmm = GMM(n_mix, random_state=self.random_state)
<             else:
<                 gmm = GMM(n_mix, covariance_type=covariance_type,
<                         random_state=self.random_state)
<             self.gmms_.append(gmm)
< 
<     def _init(self, X, lengths=None):
<         super(GMMHMM, self)._init(X, lengths=lengths)
---
>         self.gmms = gmms
>         if gmms is None:
>             gmms = []
>             for x in range(self.n_components):
>                 if covariance_type is None:
>                     g = GMM(n_mix)
>                 else:
>                     g = GMM(n_mix, covariance_type=covariance_type)
>                 gmms.append(g)
>         self.gmms_ = gmms
> 
>     # Read-only properties.
>     @property
>     def covariance_type(self):
>         """Covariance type of the model.
570,572c608,610
<         for g in self.gmms_:
<             g.set_params(init_params=self.init_params, n_iter=0)
<             g.fit(X)
---
>         Must be one of 'spherical', 'tied', 'diag', 'full'.
>         """
>         return self._covariance_type
574,575c612,613
<     def _compute_log_likelihood(self, X):
<         return np.array([g.score(X) for g in self.gmms_]).T
---
>     def _compute_log_likelihood(self, obs):
>         return np.array([g.score(obs) for g in self.gmms_]).T
579a618,625
>     def _init(self, obs, params='stwmc'):
>         super(GMMHMM, self)._init(obs, params=params)
> 
>         allobs = np.concatenate(obs, 0)
>         for g in self.gmms_:
>             g.set_params(init_params=params, n_iter=0)
>             g.fit(allobs)
> 
587,588c633,635
<     def _accumulate_sufficient_statistics(self, stats, X, framelogprob,
<                                           posteriors, fwdlattice, bwdlattice):
---
>     def _accumulate_sufficient_statistics(self, stats, obs, framelogprob,
>                                           posteriors, fwdlattice, bwdlattice,
>                                           params):
590c637,638
<             stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)
---
>             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice,
>             params)
593,595c641,643
<             lgmm_posteriors = (np.log(g.predict_proba(X))
<                                + np.log(posteriors[:, state][:, np.newaxis]
<                                         + np.finfo(np.float).eps))
---
>             _, lgmm_posteriors = g.score_samples(obs)
>             lgmm_posteriors += np.log(posteriors[:, state][:, np.newaxis]
>                                       + np.finfo(np.float).eps)
597,598d644
< 
<             n_features = g.means_.shape[1]
599a646
>             n_features = g.means_.shape[1]
604c651
<             norm = tmp_gmm._do_mstep(X, gmm_posteriors, self.params)
---
>             norm = tmp_gmm._do_mstep(obs, gmm_posteriors, params)
610c657
<             if 'm' in self.params:
---
>             if 'm' in params:
612c659
<             if 'c' in self.params:
---
>             if 'c' in params:
617c664
<                     shape = np.ones(tmp_gmm.covars_.ndim, dtype=np.int)
---
>                     shape = np.ones(tmp_gmm.covars_.ndim)
620,624c667
<                     stats['covars'][state] += (tmp_gmm.covars_
<                                                + tmp_gmm.means_**2) * cvnorm
< 
<     def _do_mstep(self, stats):
<         super(GMMHMM, self)._do_mstep(stats)
---
>                     stats['covars'][state] += tmp_gmm.covars_ * cvnorm
625a669,670
>     def _do_mstep(self, stats, params):
>         super(GMMHMM, self)._do_mstep(stats, params)
631,634c676,678
<             if 'w' in self.params:
<                 g.weights_ = norm.copy()
<                 normalize(g.weights_)
<             if 'm' in self.params:
---
>             if 'w' in params:
>                 g.weights_ = normalize(norm.copy())
>             if 'm' in params:
636c680
<             if 'c' in self.params:
---
>             if 'c' in params:
643c687
<                     shape = np.ones(g.covars_.ndim, dtype=np.int)
---
>                     shape = np.ones(g.covars_.ndim)
646c690
<                     if g.covariance_type in ['spherical', 'diag']:
---
>                     if (g.covariance_type in ['spherical', 'diag']):
648c692
<                                      self.covars_prior) / cvnorm - g.means_**2
---
>                                      self.covars_prior) / cvnorm
653c697
<                                      / cvnorm) - g.means_**2
---
>                                      / cvnorm)
